{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dc231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/YOUR_USERNAME/mpdistil.git -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef603c",
   "metadata": {},
   "source": [
    "## Method 1: From Lists of Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438852b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpdistil import MPDistil, create_simple_dataloader\n",
    "\n",
    "# Example: Sentiment analysis data\n",
    "train_texts = [\n",
    "    \"I love this movie!\",\n",
    "    \"This is terrible.\",\n",
    "    \"Great film, highly recommended.\",\n",
    "    \"Waste of time.\",\n",
    "    # Add more examples...\n",
    "]\n",
    "\n",
    "train_labels = [1, 0, 1, 0]  # 1=positive, 0=negative\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = create_simple_dataloader(\n",
    "    texts=train_texts,\n",
    "    labels=train_labels,\n",
    "    tokenizer_name='bert-base-uncased',\n",
    "    max_length=128,\n",
    "    batch_size=8,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Similarly for validation\n",
    "val_texts = [\"Amazing!\", \"Not good.\"]\n",
    "val_labels = [1, 0]\n",
    "\n",
    "val_loader = create_simple_dataloader(\n",
    "    texts=val_texts,\n",
    "    labels=val_labels,\n",
    "    tokenizer_name='bert-base-uncased',\n",
    "    max_length=128,\n",
    "    batch_size=8,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b38bcb4",
   "metadata": {},
   "source": [
    "## Method 2: From Sentence Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151817a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tasks like NLI, QA, paraphrase detection\n",
    "train_pairs = [\n",
    "    (\"The cat sat on the mat.\", \"A cat is on a mat.\"),\n",
    "    (\"It's raining heavily.\", \"The sun is shining.\"),\n",
    "    # Add more pairs...\n",
    "]\n",
    "\n",
    "train_labels = [1, 0]  # 1=entailment, 0=contradiction\n",
    "\n",
    "train_loader = create_simple_dataloader(\n",
    "    texts=train_pairs,\n",
    "    labels=train_labels,\n",
    "    tokenizer_name='bert-base-uncased',\n",
    "    max_length=128,\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee715e1",
   "metadata": {},
   "source": [
    "## Method 3: From Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26d7839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV/Excel file\n",
    "df = pd.DataFrame({\n",
    "    'text': ['Great product!', 'Disappointed', 'Excellent!'],\n",
    "    'label': [1, 0, 1]\n",
    "})\n",
    "\n",
    "# Split into train/val\n",
    "train_df = df[:2]\n",
    "val_df = df[2:]\n",
    "\n",
    "# Create loaders\n",
    "train_loader = create_simple_dataloader(\n",
    "    texts=train_df['text'].tolist(),\n",
    "    labels=train_df['label'].tolist(),\n",
    "    tokenizer_name='bert-base-uncased'\n",
    ")\n",
    "\n",
    "val_loader = create_simple_dataloader(\n",
    "    texts=val_df['text'].tolist(),\n",
    "    labels=val_df['label'].tolist(),\n",
    "    tokenizer_name='bert-base-uncased'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739d203f",
   "metadata": {},
   "source": [
    "## Method 4: Custom PyTorch Dataset (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7eeb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # IMPORTANT: Return in this exact order\n",
    "        return (\n",
    "            encoding['input_ids'].squeeze(),\n",
    "            encoding['attention_mask'].squeeze(),\n",
    "            encoding['token_type_ids'].squeeze(),\n",
    "            torch.tensor(self.labels[idx])\n",
    "        )\n",
    "\n",
    "# Create dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = CustomDataset(\n",
    "    texts=[\"Example 1\", \"Example 2\"],\n",
    "    labels=[0, 1],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Create loader\n",
    "train_loader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f9d26f",
   "metadata": {},
   "source": [
    "## Train Model with Custom Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8026fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = MPDistil(\n",
    "    task_name='MyCustomTask',\n",
    "    num_labels=2,  # Binary classification\n",
    "    student_layers=6\n",
    ")\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    teacher_epochs=5,\n",
    "    student_epochs=5\n",
    ")\n",
    "\n",
    "# Save\n",
    "model.save_student('./my_custom_student')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d883bc0e",
   "metadata": {},
   "source": [
    "## Tips for Custom Data\n",
    "\n",
    "1. **Batch Format**: Ensure your DataLoader returns tuples in this order:\n",
    "   - `(input_ids, attention_mask, token_type_ids, labels)`\n",
    "\n",
    "2. **Tokenization**: Use the same tokenizer for train/val/test\n",
    "\n",
    "3. **Max Length**: Choose based on your text length distribution\n",
    "\n",
    "4. **Label Encoding**: Labels should be integers (0, 1, 2, ...)\n",
    "\n",
    "5. **Data Split**: MPDistil auto-creates a held-out set from training data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
