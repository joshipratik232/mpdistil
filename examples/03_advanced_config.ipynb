{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbabb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/YOUR_USERNAME/mpdistil.git -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd01a9de",
   "metadata": {},
   "source": [
    "## 1. Custom Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d9247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpdistil import MPDistil, TrainingConfig, load_superglue_dataset\n",
    "\n",
    "# Create custom config\n",
    "config = TrainingConfig(\n",
    "    # Phase 1: Teacher\n",
    "    teacher_epochs=15,\n",
    "    teacher_lr=1e-5,  # Lower learning rate\n",
    "    \n",
    "    # Phase 2: Student PKD\n",
    "    student_epochs=15,\n",
    "    student_lr=5e-5,\n",
    "    alpha=0.7,        # More weight to soft targets\n",
    "    beta=50.0,        # Less PKD loss\n",
    "    temperature=3.0,  # Lower temperature\n",
    "    \n",
    "    # Phase 3: Meta-Teacher\n",
    "    meta_lr=5e-4,\n",
    "    use_competitive_loss=True,  # Try competitive instead of collaborative\n",
    "    \n",
    "    # Phase 4: Curriculum\n",
    "    num_episodes=300,\n",
    "    reward_type='real',  # Real-valued rewards\n",
    "    gamma=0.95,          # Reward discount factor\n",
    "    \n",
    "    # System\n",
    "    batch_size=16,  # Larger batch size\n",
    "    seed=12345,\n",
    "    verbose=True,\n",
    "    \n",
    "    # Checkpointing\n",
    "    output_dir='./advanced_outputs',\n",
    "    save_checkpoints=True\n",
    ")\n",
    "\n",
    "print(config.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b95d0a7",
   "metadata": {},
   "source": [
    "## 2. Multi-Task Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b116def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load main task\n",
    "cb_loaders, cb_labels = load_superglue_dataset('CB', batch_size=16)\n",
    "\n",
    "# Load auxiliary tasks for curriculum learning\n",
    "print(\"Loading auxiliary tasks...\")\n",
    "rte_loaders, _ = load_superglue_dataset('RTE', batch_size=16)\n",
    "boolq_loaders, _ = load_superglue_dataset('BoolQ', batch_size=16)\n",
    "copa_loaders, _ = load_superglue_dataset('COPA', batch_size=16)\n",
    "\n",
    "# Create model\n",
    "model = MPDistil(\n",
    "    task_name='CB',\n",
    "    num_labels=cb_labels,\n",
    "    student_layers=6\n",
    ")\n",
    "\n",
    "# Train with curriculum learning on multiple tasks\n",
    "history = model.fit(\n",
    "    train_loader=cb_loaders['train'],\n",
    "    val_loader=cb_loaders['val'],\n",
    "    meta_loaders={\n",
    "        'RTE': rte_loaders['val'],\n",
    "        'BoolQ': boolq_loaders['val'],\n",
    "        'COPA': copa_loaders['val']\n",
    "    },\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d8c82",
   "metadata": {},
   "source": [
    "## 3. Analyze Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd5748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if 'phase4' in history:\n",
    "    # Plot rewards over episodes\n",
    "    rewards = history['phase4']['rewards']\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Reward curve\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(rewards)\n",
    "    plt.title('Rewards over Episodes')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Task distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    trajectories = history['phase4']['trajectories']\n",
    "    all_tasks = [task for traj in trajectories for task in traj]\n",
    "    task_counts = {}\n",
    "    for task in all_tasks:\n",
    "        task_counts[task] = task_counts.get(task, 0) + 1\n",
    "    \n",
    "    plt.bar(task_counts.keys(), task_counts.values())\n",
    "    plt.title('Task Selection Frequency')\n",
    "    plt.xlabel('Task')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Average reward: {np.mean(rewards):.4f}\")\n",
    "    print(f\"Final reward: {rewards[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f59c02",
   "metadata": {},
   "source": [
    "## 4. Weights & Biases Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d43152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to W&B (you'll need an account)\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "# Train with W&B logging\n",
    "history = model.fit(\n",
    "    train_loader=cb_loaders['train'],\n",
    "    val_loader=cb_loaders['val'],\n",
    "    wandb_logging=True,\n",
    "    wandb_project='mpdistil-experiments',\n",
    "    teacher_epochs=5,\n",
    "    student_epochs=5\n",
    ")\n",
    "\n",
    "# View results at https://wandb.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4bdd5d",
   "metadata": {},
   "source": [
    "## 5. Different Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179cc674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoBERTa teacher → DistilBERT student\n",
    "model_roberta = MPDistil(\n",
    "    task_name='CB',\n",
    "    num_labels=3,\n",
    "    teacher_model='roberta-base',\n",
    "    student_model='distilbert-base-uncased',\n",
    "    student_layers=6\n",
    ")\n",
    "\n",
    "# Or BERT-large → BERT-base\n",
    "model_large = MPDistil(\n",
    "    task_name='CB',\n",
    "    num_labels=3,\n",
    "    teacher_model='bert-large-uncased',\n",
    "    student_model='bert-base-uncased',\n",
    "    student_layers=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5aac36",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c8220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick hyperparameter search\n",
    "alphas = [0.3, 0.5, 0.7]\n",
    "betas = [50, 100, 200]\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for alpha in alphas:\n",
    "    for beta in betas:\n",
    "        print(f\"\\nTrying alpha={alpha}, beta={beta}\")\n",
    "        \n",
    "        model = MPDistil(task_name='CB', num_labels=3, student_layers=6)\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_loader=cb_loaders['train'],\n",
    "            val_loader=cb_loaders['val'],\n",
    "            teacher_epochs=3,  # Reduce for quick search\n",
    "            student_epochs=3,\n",
    "            alpha=alpha,\n",
    "            beta=beta,\n",
    "            num_episodes=0\n",
    "        )\n",
    "        \n",
    "        # Get final score\n",
    "        final_metrics = history['phase2']['val_metrics'][-1]\n",
    "        score = final_metrics.get('acc', 0)\n",
    "        \n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = {'alpha': alpha, 'beta': beta}\n",
    "\n",
    "print(f\"\\nBest params: {best_params}\")\n",
    "print(f\"Best score: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f38af8",
   "metadata": {},
   "source": [
    "## 7. Checkpoint Resumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be418ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model state during training\n",
    "config_resume = TrainingConfig(\n",
    "    output_dir='./checkpoints',\n",
    "    save_checkpoints=True,\n",
    "    teacher_epochs=10,\n",
    "    student_epochs=10\n",
    ")\n",
    "\n",
    "# Train (will save checkpoints)\n",
    "history = model.fit(\n",
    "    train_loader=cb_loaders['train'],\n",
    "    val_loader=cb_loaders['val'],\n",
    "    config=config_resume\n",
    ")\n",
    "\n",
    "# Later, load best checkpoint\n",
    "# Note: Currently loads automatically at end of each phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e9e292",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Advanced features covered:\n",
    "- ✅ Custom TrainingConfig with all hyperparameters\n",
    "- ✅ Multi-task curriculum learning\n",
    "- ✅ Reward and trajectory analysis\n",
    "- ✅ W&B integration for experiment tracking\n",
    "- ✅ Different model architectures\n",
    "- ✅ Hyperparameter tuning strategies\n",
    "- ✅ Checkpoint management"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
